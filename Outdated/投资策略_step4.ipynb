{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如果报错就多restart几次！code没错，都是玄学"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 投资策略-step4"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T07:44:51.847665Z",
     "start_time": "2024-10-26T07:44:49.188477Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T07:44:51.850622Z",
     "start_time": "2024-10-26T07:44:51.848833Z"
    }
   },
   "cell_type": "code",
   "source": "initial_investment = 1000000\n",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T07:44:53.990738Z",
     "start_time": "2024-10-26T07:44:51.851111Z"
    }
   },
   "source": [
    "stocklist_mag = ['NFLX', 'IDR', 'AEM', 'KGC', 'IAG', 'HMY', 'RCL', 'KMI', 'PHM', 'ADSK', 'NVDA', 'RL', 'RGLD', 'DHI','ORLA','ORCL','AUST','TMUS','FSM','NGD','EXPE','NEM','PLTR']\n",
    "\n",
    "n_portfolios = 300000\n",
    "\n",
    "end_date = datetime.today()\n",
    "one_year_ago = end_date - timedelta(days=365)\n",
    "\n",
    "# Download the risk-free rate (T-bill rate from ^IRX)\n",
    "irx_data = yf.download('^IRX', start=one_year_ago, end=end_date)['Adj Close']\n",
    "# Convert ^IRX to daily risk-free rate (as IRX is annualized, divide by 252)\n",
    "daily_risk_free_rate = irx_data.mean() / 100 / 252  # Convert percentage to decimal and divide by 252 trading days\n",
    "\n",
    "risk_free_rate = daily_risk_free_rate\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T07:45:03.449661Z",
     "start_time": "2024-10-26T07:44:53.991766Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the date range\n",
    "end_date = datetime.today()\n",
    "start_date = end_date - timedelta(days=2*365)\n",
    "\n",
    "# Fetch stock data from Yahoo Finance\n",
    "data = yf.download(stocklist_mag, start=start_date, end=end_date)['Adj Close']\n",
    "\n",
    "# Calculate daily returns\n",
    "returns = data.pct_change().dropna()\n",
    "\n",
    "# Get market data for DJIA (used for calculating Treynor ratio and Jensen's Alpha)\n",
    "market_data = yf.download(\"^DJI\", start=start_date, end=end_date)['Adj Close']\n",
    "market_returns = market_data.pct_change().dropna()\n",
    "\n",
    "\n",
    "# Monte Carlo simulation parameters\n",
    "n_assets = len(stocklist_mag)\n",
    "results = np.zeros((4, n_portfolios))\n",
    "\n",
    "# Store weights\n",
    "all_weights = np.zeros((n_portfolios, n_assets))\n",
    "\n",
    "# Monte Carlo simulation\n",
    "for i in range(n_portfolios):\n",
    "    # Generate random weights\n",
    "    weights = np.random.random(n_assets)\n",
    "    weights /= np.sum(weights)\n",
    "    \n",
    "    # Save weights\n",
    "    all_weights[i, :] = weights\n",
    "    \n",
    "    # Portfolio returns and variance\n",
    "    portfolio_return = np.sum(weights * returns.mean()) * 252\n",
    "    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(returns.cov() * 252, weights)))\n",
    "    portfolio_var = portfolio_std ** 2\n",
    "    sharpe_ratio = (portfolio_return - risk_free_rate * 252) / portfolio_std\n",
    "\n",
    "    results[0, i] = portfolio_return\n",
    "    results[1, i] = portfolio_std\n",
    "    results[2, i] = sharpe_ratio\n",
    "    results[3, i] = portfolio_var\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_frame = pd.DataFrame(results.T, columns=['Return', 'StdDev', 'SharpeRatio', 'Variance'])\n",
    "\n",
    "# Find max Sharpe ratio portfolio\n",
    "max_sharpe_idx = results_frame['SharpeRatio'].idxmax()\n",
    "max_sharpe_portfolio = all_weights[max_sharpe_idx, :]\n",
    "\n",
    "# Max return\n",
    "max_return_idx = results_frame['Return'].idxmax()\n",
    "max_return_portfolio = all_weights[max_return_idx, :]\n",
    "\n",
    "# Min variance\n",
    "min_variance_idx = results_frame['Variance'].idxmin()\n",
    "min_variance_portfolio = all_weights[min_variance_idx, :]\n",
    "\n",
    "# Treynor and Jensen's Alpha (use for optimized portfolios only)\n",
    "def treynor_ratio(returns, weights, beta, market_return, risk_free_rate):\n",
    "    portfolio_return = np.sum(weights * returns.mean()) * 252\n",
    "    treynor = (portfolio_return - risk_free_rate * 252) / beta\n",
    "    return treynor\n",
    "\n",
    "def jensens_alpha(returns, weights, beta, market_return, risk_free_rate):\n",
    "    portfolio_return = np.sum(weights * returns.mean()) * 252\n",
    "    alpha = portfolio_return - (risk_free_rate * 252 + beta * (market_return.mean() - risk_free_rate * 252))\n",
    "    return alpha\n",
    "\n",
    "# Calculate portfolio betas\n",
    "beta_values = np.zeros(n_portfolios)\n",
    "market_excess_return = market_returns.mean() - risk_free_rate * 252\n",
    "\n",
    "for i in range(n_portfolios):\n",
    "    portfolio_beta = np.dot(all_weights[i], np.cov(returns.T, market_returns.T)[:-1, -1]) / market_returns.var()\n",
    "    beta_values[i] = portfolio_beta\n",
    "\n",
    "# Max Treynor Ratio\n",
    "treynor_ratios = np.array([treynor_ratio(returns, all_weights[i], beta_values[i], market_returns, risk_free_rate)\n",
    "                           for i in range(n_portfolios)])\n",
    "max_treynor_idx = np.argmax(treynor_ratios)\n",
    "max_treynor_portfolio = all_weights[max_treynor_idx, :]\n",
    "\n",
    "# Max Jensen's Alpha\n",
    "jensen_alphas = np.array([jensens_alpha(returns, all_weights[i], beta_values[i], market_returns, risk_free_rate)\n",
    "                          for i in range(n_portfolios)])\n",
    "max_jensen_alpha_idx = np.argmax(jensen_alphas)\n",
    "max_jensen_alpha_portfolio = all_weights[max_jensen_alpha_idx, :]\n",
    "\n",
    "# Display Results\n",
    "portfolios = {\n",
    "    'Max Sharpe Ratio': max_sharpe_portfolio,\n",
    "    'Max Treynor Ratio': max_treynor_portfolio,\n",
    "    'Max Jensen Alpha': max_jensen_alpha_portfolio,\n",
    "    'Max Return': max_return_portfolio,\n",
    "    'Min Variance': min_variance_portfolio\n",
    "}\n",
    "\n",
    "for name, portfolio in portfolios.items():\n",
    "    portfolio_return = np.sum(portfolio * returns.mean()) * 252\n",
    "    portfolio_std = np.sqrt(np.dot(portfolio.T, np.dot(returns.cov() * 252, portfolio)))\n",
    "    print(f\"\\n{name} Portfolio:\")\n",
    "    print(f\"Return: {portfolio_return}\")\n",
    "    print(f\"Std Dev: {portfolio_std}\")\n",
    "    print(f\"Weights: {portfolio}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  23 of 23 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 40\u001B[0m\n\u001B[1;32m     37\u001B[0m all_weights[i, :] \u001B[38;5;241m=\u001B[39m weights\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# Portfolio returns and variance\u001B[39;00m\n\u001B[0;32m---> 40\u001B[0m portfolio_return \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(weights \u001B[38;5;241m*\u001B[39m \u001B[43mreturns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m252\u001B[39m\n\u001B[1;32m     41\u001B[0m portfolio_std \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msqrt(np\u001B[38;5;241m.\u001B[39mdot(weights\u001B[38;5;241m.\u001B[39mT, np\u001B[38;5;241m.\u001B[39mdot(returns\u001B[38;5;241m.\u001B[39mcov() \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m252\u001B[39m, weights)))\n\u001B[1;32m     42\u001B[0m portfolio_var \u001B[38;5;241m=\u001B[39m portfolio_std \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/副业/1011-2/1012_port_stock/.venv/lib/python3.12/site-packages/pandas/core/frame.py:11693\u001B[0m, in \u001B[0;36mDataFrame.mean\u001B[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001B[0m\n\u001B[1;32m  11685\u001B[0m \u001B[38;5;129m@doc\u001B[39m(make_doc(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmean\u001B[39m\u001B[38;5;124m\"\u001B[39m, ndim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m))\n\u001B[1;32m  11686\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmean\u001B[39m(\n\u001B[1;32m  11687\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m  11691\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m  11692\u001B[0m ):\n\u001B[0;32m> 11693\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskipna\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumeric_only\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m  11694\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, Series):\n\u001B[1;32m  11695\u001B[0m         result \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmean\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/副业/1011-2/1012_port_stock/.venv/lib/python3.12/site-packages/pandas/core/generic.py:12420\u001B[0m, in \u001B[0;36mNDFrame.mean\u001B[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001B[0m\n\u001B[1;32m  12413\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmean\u001B[39m(\n\u001B[1;32m  12414\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m  12415\u001B[0m     axis: Axis \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m  12418\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m  12419\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Series \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[0;32m> 12420\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stat_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m  12421\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmean\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnanops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnanmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskipna\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumeric_only\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m  12422\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/副业/1011-2/1012_port_stock/.venv/lib/python3.12/site-packages/pandas/core/generic.py:12377\u001B[0m, in \u001B[0;36mNDFrame._stat_function\u001B[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001B[0m\n\u001B[1;32m  12373\u001B[0m nv\u001B[38;5;241m.\u001B[39mvalidate_func(name, (), kwargs)\n\u001B[1;32m  12375\u001B[0m validate_bool_kwarg(skipna, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskipna\u001B[39m\u001B[38;5;124m\"\u001B[39m, none_allowed\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m> 12377\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reduce\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m  12378\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskipna\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskipna\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumeric_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnumeric_only\u001B[49m\n\u001B[1;32m  12379\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/副业/1011-2/1012_port_stock/.venv/lib/python3.12/site-packages/pandas/core/frame.py:11562\u001B[0m, in \u001B[0;36mDataFrame._reduce\u001B[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001B[0m\n\u001B[1;32m  11558\u001B[0m     df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mT\n\u001B[1;32m  11560\u001B[0m \u001B[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001B[39;00m\n\u001B[1;32m  11561\u001B[0m \u001B[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001B[39;00m\n\u001B[0;32m> 11562\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mgr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblk_func\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m  11563\u001B[0m out \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39m_constructor_from_mgr(res, axes\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39maxes)\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m  11564\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m out_dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m out\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mboolean\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/副业/1011-2/1012_port_stock/.venv/lib/python3.12/site-packages/pandas/core/internals/managers.py:1500\u001B[0m, in \u001B[0;36mBlockManager.reduce\u001B[0;34m(self, func)\u001B[0m\n\u001B[1;32m   1498\u001B[0m res_blocks: \u001B[38;5;28mlist\u001B[39m[Block] \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m   1499\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks:\n\u001B[0;32m-> 1500\u001B[0m     nbs \u001B[38;5;241m=\u001B[39m \u001B[43mblk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1501\u001B[0m     res_blocks\u001B[38;5;241m.\u001B[39mextend(nbs)\n\u001B[1;32m   1503\u001B[0m index \u001B[38;5;241m=\u001B[39m Index([\u001B[38;5;28;01mNone\u001B[39;00m])  \u001B[38;5;66;03m# placeholder\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/副业/1011-2/1012_port_stock/.venv/lib/python3.12/site-packages/pandas/core/internals/blocks.py:404\u001B[0m, in \u001B[0;36mBlock.reduce\u001B[0;34m(self, func)\u001B[0m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreduce\u001B[39m(\u001B[38;5;28mself\u001B[39m, func) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[Block]:\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001B[39;00m\n\u001B[1;32m    401\u001B[0m     \u001B[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001B[39;00m\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m--> 404\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    406\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    407\u001B[0m         res_values \u001B[38;5;241m=\u001B[39m result\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/副业/1011-2/1012_port_stock/.venv/lib/python3.12/site-packages/pandas/core/frame.py:11481\u001B[0m, in \u001B[0;36mDataFrame._reduce.<locals>.blk_func\u001B[0;34m(values, axis)\u001B[0m\n\u001B[1;32m  11479\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray([result])\n\u001B[1;32m  11480\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m> 11481\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskipna\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskipna\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/副业/1011-2/1012_port_stock/.venv/lib/python3.12/site-packages/pandas/core/nanops.py:147\u001B[0m, in \u001B[0;36mbottleneck_switch.__call__.<locals>.f\u001B[0;34m(values, axis, skipna, **kwds)\u001B[0m\n\u001B[1;32m    145\u001B[0m         result \u001B[38;5;241m=\u001B[39m alt(values, axis\u001B[38;5;241m=\u001B[39maxis, skipna\u001B[38;5;241m=\u001B[39mskipna, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m    146\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 147\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43malt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskipna\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskipna\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/副业/1011-2/1012_port_stock/.venv/lib/python3.12/site-packages/pandas/core/nanops.py:404\u001B[0m, in \u001B[0;36m_datetimelike_compat.<locals>.new_func\u001B[0;34m(values, axis, skipna, mask, **kwargs)\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m datetimelike \u001B[38;5;129;01mand\u001B[39;00m mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    402\u001B[0m     mask \u001B[38;5;241m=\u001B[39m isna(values)\n\u001B[0;32m--> 404\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskipna\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskipna\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    406\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m datetimelike:\n\u001B[1;32m    407\u001B[0m     result \u001B[38;5;241m=\u001B[39m _wrap_results(result, orig_values\u001B[38;5;241m.\u001B[39mdtype, fill_value\u001B[38;5;241m=\u001B[39miNaT)\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/副业/1011-2/1012_port_stock/.venv/lib/python3.12/site-packages/pandas/core/nanops.py:728\u001B[0m, in \u001B[0;36mnanmean\u001B[0;34m(values, axis, skipna, mask)\u001B[0m\n\u001B[1;32m    726\u001B[0m         the_mean \u001B[38;5;241m=\u001B[39m the_sum \u001B[38;5;241m/\u001B[39m count\n\u001B[1;32m    727\u001B[0m     ct_mask \u001B[38;5;241m=\u001B[39m count \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 728\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mct_mask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43many\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    729\u001B[0m         the_mean[ct_mask] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mnan\n\u001B[1;32m    730\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/副业/1011-2/1012_port_stock/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:65\u001B[0m, in \u001B[0;36m_any\u001B[0;34m(a, axis, dtype, out, keepdims, where)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m where \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mumr_any\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m umr_any(a, axis, dtype, out, keepdims, where\u001B[38;5;241m=\u001B[39mwhere)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T07:45:03.451170Z",
     "start_time": "2024-10-26T07:45:03.451118Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "end_date = datetime.today()\n",
    "one_year_ago = end_date - timedelta(days=2* 365)\n",
    "backtest_data = data\n",
    "\n",
    "\n",
    "# Step 2: Define the portfolio value calculation function\n",
    "def portfolio_value(weights, stock_data, initial_investment):\n",
    "    investment_per_stock = initial_investment * weights\n",
    "    shares = investment_per_stock / stock_data.iloc[0]\n",
    "    portfolio_value = (stock_data * shares).sum(axis=1)\n",
    "    return portfolio_value\n",
    "\n",
    "# Step 3: Implement Portfolio 1 (Monte Carlo Simulation with combined weights)\n",
    "weights_combined = {\n",
    "    'Max Return': 0.20,\n",
    "    'Min Variance': 0.30,\n",
    "    'Max Sharpe Ratio': 0.20,\n",
    "    'Max Treynor Ratio': 0.15,\n",
    "    'Max Jensen Alpha': 0.15\n",
    "}\n",
    "\n",
    "portfolios = {\n",
    "    'Max Return': np.random.dirichlet(np.ones(len(stocklist_mag))),\n",
    "    'Min Variance': np.random.dirichlet(np.ones(len(stocklist_mag))),\n",
    "    'Max Sharpe Ratio': np.random.dirichlet(np.ones(len(stocklist_mag))),\n",
    "    'Max Treynor Ratio': np.random.dirichlet(np.ones(len(stocklist_mag))),\n",
    "    'Max Jensen Alpha': np.random.dirichlet(np.ones(len(stocklist_mag)))\n",
    "}\n",
    "\n",
    "combined_weights = np.zeros(len(stocklist_mag))\n",
    "for name, weight in weights_combined.items():\n",
    "    combined_weights += portfolios[name] * weight\n",
    "combined_weights /= np.sum(combined_weights)\n",
    "\n",
    "combined_portfolio_value = portfolio_value(combined_weights, backtest_data, initial_investment)\n",
    "\n",
    "# Step 4: Implement Portfolio 2 (Equal weighting of three professional optimizations)\n",
    "optimized_portfolio_values = {}\n",
    "\n",
    "def portfolio_performance(weights, returns):\n",
    "    portfolio_return = np.sum(weights * returns.mean()) * 252\n",
    "    portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(returns.cov() * 252, weights)))\n",
    "    return portfolio_return, portfolio_volatility\n",
    "\n",
    "# Define bounds to allow zero weights but not negative values (0 to 1)\n",
    "bounds = tuple((0, 1) for asset in range(returns.shape[1]))\n",
    "\n",
    "# Mean-Variance Optimization\n",
    "def min_variance_portfolio(returns):\n",
    "    num_assets = returns.shape[1]\n",
    "    constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})\n",
    "\n",
    "    def portfolio_volatility(weights, returns):\n",
    "        return portfolio_performance(weights, returns)[1]\n",
    "\n",
    "    result = minimize(portfolio_volatility, num_assets * [1. / num_assets,], args=(returns,),\n",
    "                      method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    return result.x\n",
    "\n",
    "# Risk Parity Optimization\n",
    "def risk_parity_portfolio(returns):\n",
    "    cov_matrix = returns.cov()\n",
    "\n",
    "    def risk_budget_objective(weights):\n",
    "        total_portfolio_variance = np.dot(weights.T, np.dot(cov_matrix, weights))\n",
    "        marginal_risk_contribution = np.dot(cov_matrix, weights)\n",
    "        risk_contribution = weights * marginal_risk_contribution / total_portfolio_variance\n",
    "        return np.sum((risk_contribution - 1.0 / len(weights)) ** 2)\n",
    "\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "\n",
    "    result = minimize(risk_budget_objective, np.ones(returns.shape[1]) / returns.shape[1],\n",
    "                      bounds=bounds, constraints=constraints)\n",
    "    return result.x\n",
    "\n",
    "# Max Sharpe Ratio Optimization\n",
    "def max_sharpe_ratio_portfolio(returns, risk_free_rate):\n",
    "    num_assets = returns.shape[1]\n",
    "    constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})\n",
    "\n",
    "    def negative_sharpe_ratio(weights, returns, risk_free_rate):\n",
    "        portfolio_return, portfolio_volatility = portfolio_performance(weights, returns)\n",
    "        return -(portfolio_return - risk_free_rate * 252) / portfolio_volatility\n",
    "\n",
    "    result = minimize(negative_sharpe_ratio, num_assets * [1. / num_assets,], args=(returns, risk_free_rate),\n",
    "                      method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    return result.x\n",
    "\n",
    "\n",
    "# Generate weights for each optimization strategy\n",
    "mean_variance_weights = min_variance_portfolio(returns)\n",
    "risk_parity_weights = risk_parity_portfolio(returns)\n",
    "max_sharpe_weights = max_sharpe_ratio_portfolio(returns, daily_risk_free_rate)\n",
    "\n",
    "# Combine the three optimized portfolios equally\n",
    "combined_optimized_weights = (mean_variance_weights + risk_parity_weights + max_sharpe_weights) / 3\n",
    "\n",
    "# Backtest Portfolio 2 (Combined Professional Optimized)\n",
    "optimized_portfolio_values['Combined Professional'] = portfolio_value(combined_optimized_weights, backtest_data, initial_investment)\n",
    "\n",
    "# Step 6: Fetch major indexes for comparison\n",
    "index_symbols = ['^GSPC', '^DJI', '^IXIC']\n",
    "index_data = yf.download(index_symbols, start=one_year_ago, end=end_date)['Adj Close']\n",
    "\n",
    "# Step 7: Plot the comparison of all portfolios and major indexes\n",
    "def normalize(values):\n",
    "    return values / values.iloc[0]\n",
    "\n",
    "normalized_combined_portfolio = normalize(combined_portfolio_value)\n",
    "normalized_indexes = {\n",
    "    'S&P 500': normalize(index_data['^GSPC']),\n",
    "    'Dow Jones': normalize(index_data['^DJI']),\n",
    "    'Nasdaq': normalize(index_data['^IXIC'])\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Portfolio 1 (Monte Carlo Simulation)\n",
    "plt.plot(normalize(combined_portfolio_value).index, normalize(combined_portfolio_value), label='Portfolio 1 (Monte Carlo)', color='b')\n",
    "\n",
    "# Plot Portfolio 2 (Combined Professional Optimizations)\n",
    "plt.plot(normalize(optimized_portfolio_values['Combined Professional']).index, normalize(optimized_portfolio_values['Combined Professional']), \n",
    "         label='Portfolio 2 (Combined Professional)', color='g')\n",
    "\n",
    "# Plot each major index\n",
    "for name, index_values in normalized_indexes.items():\n",
    "    plt.plot(index_values.index, normalize(index_values), label=name)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Portfolio Comparison (1, 2, 3) vs Major Indexes')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Normalized Value (Starting at 1)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from scipy.optimize import minimize\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from prophet import Prophet\n",
    "\n",
    "end_date = datetime.today()\n",
    "one_year_ago = end_date - timedelta(days=2 * 365)\n",
    "backtest_data = data  # Assuming 'data' contains stock price data\n",
    "\n",
    "# Step 2: Define the portfolio value calculation function\n",
    "def portfolio_value(weights, stock_data, initial_investment):\n",
    "    investment_per_stock = initial_investment * weights\n",
    "    shares = investment_per_stock / stock_data.iloc[0]\n",
    "    portfolio_value = (stock_data * shares).sum(axis=1)\n",
    "    return portfolio_value\n",
    "\n",
    "# RandomForestRegressor Method with Cross-Validation\n",
    "def prepare_ml_data(returns):\n",
    "    data = returns.shift(1).dropna()\n",
    "    target = returns.loc[data.index]  # Ensure target aligns with the shifted data\n",
    "    return data, target\n",
    "\n",
    "X, y = prepare_ml_data(returns)\n",
    "\n",
    "# Use TimeSeriesSplit for Cross-Validation to avoid look-ahead bias\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rf_model = RandomForestRegressor(n_estimators=5000, random_state=42)\n",
    "\n",
    "# Perform cross-validation and train the model\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "predicted_returns = rf_model.predict(returns)\n",
    "predicted_returns_df = pd.DataFrame(predicted_returns, index=returns.index, columns=returns.columns)\n",
    "\n",
    "# Covariance matrix for portfolio risk\n",
    "cov_matrix = returns.cov()\n",
    "\n",
    "# Updated optimization function using Sharpe ratio\n",
    "def optimized_portfolio_using_predictions(predicted_returns):\n",
    "    num_assets = predicted_returns.shape[1]\n",
    "    args = (predicted_returns.mean(), cov_matrix)\n",
    "    constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})\n",
    "    bounds = tuple((0, 0.2) for asset in range(num_assets))  # Limit weights to 20% max per stock\n",
    "\n",
    "    def negative_sharpe_ratio(weights, predicted_returns, cov_matrix):\n",
    "        portfolio_return = np.sum(weights * predicted_returns) * 252\n",
    "        portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)\n",
    "        sharpe_ratio = portfolio_return / portfolio_volatility\n",
    "        return -sharpe_ratio  # Minimize the negative Sharpe ratio\n",
    "\n",
    "    result = minimize(negative_sharpe_ratio, num_assets * [1. / num_assets,], args=args,\n",
    "                      method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    return result.x\n",
    "\n",
    "ml_optimized_weights = optimized_portfolio_using_predictions(predicted_returns_df)\n",
    "optimized_portfolio_values = {}\n",
    "optimized_portfolio_values['ML Optimized'] = portfolio_value(ml_optimized_weights, backtest_data, initial_investment)\n",
    "\n",
    "# Step 5: Implement Portfolio 4 (LSTM)\n",
    "def prepare_lstm_data(returns):\n",
    "    data = returns.values\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - 60):\n",
    "        X.append(data[i:i+60])  # 60 days of lookback\n",
    "        y.append(data[i + 60])  # Predict the next day\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_lstm, y_lstm = prepare_lstm_data(returns)\n",
    "\n",
    "# Define the LSTM model with reduced lookback and dropout\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(X_lstm.shape[1], X_lstm.shape[2])))\n",
    "lstm_model.add(LSTM(50, activation='relu'))\n",
    "lstm_model.add(Dense(y_lstm.shape[1]))  # Output layer with same number of assets as input\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the LSTM model\n",
    "lstm_model.fit(X_lstm, y_lstm, epochs=50, batch_size=32)\n",
    "\n",
    "# Predict using the LSTM model\n",
    "predicted_lstm_returns = lstm_model.predict(X_lstm[-1].reshape(1, X_lstm.shape[1], X_lstm.shape[2]))[0]\n",
    "\n",
    "# LSTM optimization with Sharpe ratio\n",
    "def optimized_portfolio_using_lstm(predicted_returns):\n",
    "    num_assets = predicted_returns.shape[0]\n",
    "    args = (predicted_returns, cov_matrix)\n",
    "    constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})\n",
    "    bounds = tuple((0, 0.2) for asset in range(num_assets))\n",
    "\n",
    "    def negative_sharpe_ratio(weights, predicted_returns, cov_matrix):\n",
    "        portfolio_return = np.sum(weights * predicted_returns) * 252\n",
    "        portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)\n",
    "        sharpe_ratio = portfolio_return / portfolio_volatility\n",
    "        return -sharpe_ratio\n",
    "\n",
    "    result = minimize(negative_sharpe_ratio, num_assets * [1. / num_assets,], args=args,\n",
    "                      method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    return result.x\n",
    "\n",
    "lstm_optimized_weights = optimized_portfolio_using_lstm(predicted_lstm_returns)\n",
    "optimized_portfolio_values['LSTM Optimized'] = portfolio_value(lstm_optimized_weights, backtest_data, initial_investment)\n",
    "\n",
    "# Step 6: Implement Portfolio 5 (Prophet)\n",
    "def prepare_prophet_data(returns):\n",
    "    df = pd.DataFrame({\n",
    "        'ds': returns.index,\n",
    "        'y': returns.mean(axis=1)  # Use the average return for prediction\n",
    "    }).reset_index(drop=True)\n",
    "    df['ds'] = df['ds'].dt.tz_localize(None)  # Remove timezone information from 'ds' column\n",
    "    return df\n",
    "\n",
    "prophet_data = prepare_prophet_data(returns)\n",
    "\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(prophet_data)\n",
    "\n",
    "# Forecast for the next 360 periods\n",
    "future = prophet_model.make_future_dataframe(periods=360)\n",
    "forecast = prophet_model.predict(future)\n",
    "\n",
    "# Prophet optimization using Sharpe ratio\n",
    "def optimized_portfolio_using_prophet(forecasted_returns):\n",
    "    num_assets = returns.shape[1]  # Number of assets in your portfolio\n",
    "    args = (forecasted_returns, cov_matrix)\n",
    "    constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})\n",
    "    bounds = tuple((0, 0.2) for _ in range(num_assets))\n",
    "    \n",
    "    expected_returns = np.repeat(forecasted_returns['yhat'].values[-360:].mean(), num_assets)\n",
    "\n",
    "    def negative_sharpe_ratio(weights, expected_returns, cov_matrix):\n",
    "        portfolio_return = np.sum(weights * expected_returns) * 252\n",
    "        portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)\n",
    "        sharpe_ratio = portfolio_return / portfolio_volatility\n",
    "        return -sharpe_ratio\n",
    "\n",
    "    result = minimize(negative_sharpe_ratio, num_assets * [1. / num_assets,], args=(expected_returns, cov_matrix),\n",
    "                      method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    return result.x\n",
    "\n",
    "prophet_optimized_weights = optimized_portfolio_using_prophet(forecast)\n",
    "optimized_portfolio_values['Prophet Optimized'] = portfolio_value(prophet_optimized_weights, backtest_data, initial_investment)\n",
    "\n",
    "# Step 7: Fetch major indexes for comparison\n",
    "index_symbols = ['^GSPC', '^DJI', '^IXIC']\n",
    "index_data = yf.download(index_symbols, start=one_year_ago, end=end_date)['Adj Close']\n",
    "\n",
    "# Step 8: Plot the comparison of all portfolios and major indexes\n",
    "def normalize(values):\n",
    "    return values / values.iloc[0]\n",
    "\n",
    "normalized_combined_portfolio = normalize(optimized_portfolio_values['ML Optimized'])\n",
    "normalized_indexes = {\n",
    "    'S&P 500': normalize(index_data['^GSPC']),\n",
    "    'Dow Jones': normalize(index_data['^DJI']),\n",
    "    'Nasdaq': normalize(index_data['^IXIC'])\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the portfolios\n",
    "plt.plot(normalized_combined_portfolio.index, normalized_combined_portfolio, label='ML Optimized', color='r')\n",
    "plt.plot(normalize(optimized_portfolio_values['LSTM Optimized']).index, normalize(optimized_portfolio_values['LSTM Optimized']), \n",
    "         label='LSTM Optimized', color='m')\n",
    "plt.plot(normalize(optimized_portfolio_values['Prophet Optimized']).index, normalize(optimized_portfolio_values['Prophet Optimized']), \n",
    "         label='Prophet Optimized', color='c')\n",
    "\n",
    "# Plot each major index\n",
    "for name, index_values in normalized_indexes.items():\n",
    "    plt.plot(index_values.index, index_values, label=name)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Portfolio Comparison (ML, LSTM, Prophet) vs Major Indexes')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Normalized Value (Starting at 1)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from scipy.optimize import minimize\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from prophet import Prophet\n",
    "\n",
    "# Set date range for backtesting\n",
    "end_date = datetime.today()\n",
    "one_year_ago = end_date - timedelta(days=2 * 365)\n",
    "backtest_data = data  # Assuming 'data' contains stock price data\n",
    "\n",
    "# Portfolio value calculation function\n",
    "def portfolio_value(weights, stock_data, initial_investment):\n",
    "    investment_per_stock = initial_investment * weights\n",
    "    shares = investment_per_stock / stock_data.iloc[0]\n",
    "    portfolio_value = (stock_data * shares).sum(axis=1)\n",
    "    return portfolio_value\n",
    "\n",
    "# RandomForestRegressor Method with Cross-Validation\n",
    "def prepare_ml_data(returns):\n",
    "    data = returns.shift(1).dropna()\n",
    "    target = returns.loc[data.index]  # Ensure target aligns with the shifted data\n",
    "    return data, target\n",
    "\n",
    "X, y = prepare_ml_data(returns)\n",
    "\n",
    "# TimeSeriesSplit for Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rf_model = RandomForestRegressor(n_estimators=5000, random_state=42)\n",
    "\n",
    "# Cross-validation and train the model\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "predicted_returns = rf_model.predict(returns)\n",
    "predicted_returns_df = pd.DataFrame(predicted_returns, index=returns.index, columns=returns.columns)\n",
    "\n",
    "# Covariance matrix for portfolio risk\n",
    "cov_matrix = returns.cov()\n",
    "\n",
    "# Optimization using Sharpe ratio\n",
    "def optimized_portfolio_using_predictions(predicted_returns):\n",
    "    num_assets = predicted_returns.shape[1]\n",
    "    args = (predicted_returns.mean(), cov_matrix)\n",
    "    constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})\n",
    "    bounds = tuple((0, 0.2) for asset in range(num_assets))\n",
    "\n",
    "    def negative_sharpe_ratio(weights, predicted_returns, cov_matrix):\n",
    "        portfolio_return = np.sum(weights * predicted_returns) * 252\n",
    "        portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)\n",
    "        sharpe_ratio = portfolio_return / portfolio_volatility\n",
    "        return -sharpe_ratio\n",
    "\n",
    "    result = minimize(negative_sharpe_ratio, num_assets * [1. / num_assets,], args=args,\n",
    "                      method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    return result.x\n",
    "\n",
    "ml_optimized_weights = optimized_portfolio_using_predictions(predicted_returns_df)\n",
    "optimized_portfolio_values = {}\n",
    "optimized_portfolio_values['ML Optimized'] = portfolio_value(ml_optimized_weights, backtest_data, initial_investment)\n",
    "\n",
    "# LSTM Portfolio\n",
    "def prepare_lstm_data(returns):\n",
    "    data = returns.values\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - 60):\n",
    "        X.append(data[i:i+60])  # 60 days of lookback\n",
    "        y.append(data[i + 60])  # Predict the next day\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_lstm, y_lstm = prepare_lstm_data(returns)\n",
    "\n",
    "# Define the LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(X_lstm.shape[1], X_lstm.shape[2])))\n",
    "lstm_model.add(LSTM(50, activation='relu'))\n",
    "lstm_model.add(Dense(y_lstm.shape[1]))  # Output layer with same number of assets as input\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the LSTM model\n",
    "lstm_model.fit(X_lstm, y_lstm, epochs=50, batch_size=32)\n",
    "\n",
    "# Predict using the LSTM model\n",
    "predicted_lstm_returns = lstm_model.predict(X_lstm[-1].reshape(1, X_lstm.shape[1], X_lstm.shape[2]))[0]\n",
    "\n",
    "# Ensure predicted_lstm_returns is a 2D array with shape (1, num_assets)\n",
    "predicted_lstm_returns = np.reshape(predicted_lstm_returns, (1, -1))\n",
    "\n",
    "# Now pass the reshaped returns to the optimization function\n",
    "lstm_optimized_weights = optimized_portfolio_using_predictions(pd.DataFrame(predicted_lstm_returns))\n",
    "optimized_portfolio_values['LSTM Optimized'] = portfolio_value(lstm_optimized_weights, backtest_data, initial_investment)\n",
    "\n",
    "\n",
    "# Prophet Portfolio\n",
    "def prepare_prophet_data(returns):\n",
    "    df = pd.DataFrame({\n",
    "        'ds': returns.index,\n",
    "        'y': returns.mean(axis=1)  # Use the average return for prediction\n",
    "    }).reset_index(drop=True)\n",
    "    df['ds'] = df['ds'].dt.tz_localize(None)\n",
    "    return df\n",
    "\n",
    "prophet_data = prepare_prophet_data(returns)\n",
    "\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(prophet_data)\n",
    "\n",
    "# Forecast for the next 360 periods\n",
    "future = prophet_model.make_future_dataframe(periods=360)\n",
    "forecast = prophet_model.predict(future)\n",
    "\n",
    "# Prophet optimization using Sharpe ratio\n",
    "def optimized_portfolio_using_prophet(forecasted_returns):\n",
    "    num_assets = returns.shape[1]  # Number of assets in your portfolio\n",
    "    expected_returns = np.repeat(forecasted_returns['yhat'].values[-360:].mean(), num_assets)\n",
    "    \n",
    "    # Define bounds to limit the weight of each asset\n",
    "    bounds = tuple((0, 0.2) for _ in range(num_assets))  # Limiting weights to max 20% per asset\n",
    "\n",
    "    def negative_sharpe_ratio(weights, expected_returns, cov_matrix):\n",
    "        portfolio_return = np.sum(weights * expected_returns) * 252\n",
    "        portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)\n",
    "        sharpe_ratio = portfolio_return / portfolio_volatility\n",
    "        return -sharpe_ratio\n",
    "\n",
    "    constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})  # Sum of weights must equal 1\n",
    "\n",
    "    result = minimize(negative_sharpe_ratio, num_assets * [1. / num_assets,], args=(expected_returns, cov_matrix),\n",
    "                      method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    return result.x\n",
    "\n",
    "# Use this updated function\n",
    "prophet_optimized_weights = optimized_portfolio_using_prophet(forecast)\n",
    "optimized_portfolio_values['Prophet Optimized'] = portfolio_value(prophet_optimized_weights, backtest_data, initial_investment)\n",
    "\n",
    "# Modify Monte Carlo Simulation with combined weights to allow 0 weight\n",
    "weights_combined = {\n",
    "    'Max Return': 0.20,\n",
    "    'Min Variance': 0.30,\n",
    "    'Max Sharpe Ratio': 0.20,\n",
    "    'Max Treynor Ratio': 0.15,\n",
    "    'Max Jensen Alpha': 0.15\n",
    "}\n",
    "\n",
    "# Generate portfolios using np.random.uniform to allow 0 weights and normalize\n",
    "portfolios = {\n",
    "    'Max Return': np.random.uniform(0, 1, len(stocklist_mag)),\n",
    "    'Min Variance': np.random.uniform(0, 1, len(stocklist_mag)),\n",
    "    'Max Sharpe Ratio': np.random.uniform(0, 1, len(stocklist_mag)),\n",
    "    'Max Treynor Ratio': np.random.uniform(0, 1, len(stocklist_mag)),\n",
    "    'Max Jensen Alpha': np.random.uniform(0, 1, len(stocklist_mag))\n",
    "}\n",
    "\n",
    "# Normalize weights to sum to 1\n",
    "for name in portfolios:\n",
    "    portfolios[name] /= np.sum(portfolios[name])\n",
    "\n",
    "# Combine the weights\n",
    "combined_weights = np.zeros(len(stocklist_mag))\n",
    "for name, weight in weights_combined.items():\n",
    "    combined_weights += portfolios[name] * weight\n",
    "combined_weights /= np.sum(combined_weights)\n",
    "\n",
    "combined_portfolio_value = portfolio_value(combined_weights, backtest_data, initial_investment)\n",
    "\n",
    "\n",
    "# Equal weighting of three professional optimizations\n",
    "mean_variance_weights = min_variance_portfolio(returns)\n",
    "risk_parity_weights = risk_parity_portfolio(returns)\n",
    "max_sharpe_weights = max_sharpe_ratio_portfolio(returns, daily_risk_free_rate)\n",
    "\n",
    "combined_optimized_weights = (mean_variance_weights + risk_parity_weights + max_sharpe_weights) / 3\n",
    "optimized_portfolio_values['Combined Professional'] = portfolio_value(combined_optimized_weights, backtest_data, initial_investment)\n",
    "\n",
    "# Fetch major indexes for comparison\n",
    "index_symbols = ['^GSPC', '^DJI', '^IXIC']\n",
    "index_data = yf.download(index_symbols, start=one_year_ago, end=end_date)['Adj Close']\n",
    "\n",
    "# Plot comparison of all portfolios and major indexes\n",
    "def normalize(values):\n",
    "    return values / values.iloc[0]\n",
    "\n",
    "normalized_combined_portfolio = normalize(optimized_portfolio_values['ML Optimized'])\n",
    "normalized_indexes = {\n",
    "    'S&P 500': normalize(index_data['^GSPC']),\n",
    "    'Dow Jones': normalize(index_data['^DJI']),\n",
    "    'Nasdaq': normalize(index_data['^IXIC'])\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot portfolios\n",
    "plt.plot(normalized_combined_portfolio.index, normalized_combined_portfolio, label='ML Optimized', color='r')\n",
    "plt.plot(normalize(optimized_portfolio_values['LSTM Optimized']).index, normalize(optimized_portfolio_values['LSTM Optimized']), label='LSTM Optimized', color='m')\n",
    "plt.plot(normalize(optimized_portfolio_values['Prophet Optimized']).index, normalize(optimized_portfolio_values['Prophet Optimized']), label='Prophet Optimized', color='c')\n",
    "plt.plot(normalize(combined_portfolio_value).index, normalize(combined_portfolio_value), label='Portfolio 1 (Monte Carlo)', color='b')\n",
    "plt.plot(normalize(optimized_portfolio_values['Combined Professional']).index, normalize(optimized_portfolio_values['Combined Professional']), label='Portfolio 2 (Combined Professional)', color='g')\n",
    "\n",
    "# Plot each major index\n",
    "for name, index_values in normalized_indexes.items():\n",
    "    plt.plot(index_values.index, index_values, label=name)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Portfolio Comparison vs Major Indexes')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Normalized Value (Starting at 1)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 1: Check optimized weights for each strategy\n",
    "optimized_weights = {\n",
    "    'ML Optimized': ml_optimized_weights,\n",
    "    'LSTM Optimized': lstm_optimized_weights,\n",
    "    'Prophet Optimized': prophet_optimized_weights,\n",
    "    'Monte Carlo (Combined Weights)': combined_weights,\n",
    "    'Combined Professional Optimizations': combined_optimized_weights\n",
    "}\n",
    "\n",
    "# Step 2: Function to check if zero weights are allowed\n",
    "def check_zero_weights(weights, strategy_name):\n",
    "    zero_weights = np.isclose(weights, 0)  # Check if any weights are approximately zero\n",
    "    if np.any(zero_weights):\n",
    "        print(f\"{strategy_name} allows zero weights.\")\n",
    "    else:\n",
    "        print(f\"{strategy_name} does not allow zero weights.\")\n",
    "\n",
    "# Step 3: Check for zero weights in each strategy\n",
    "for strategy, weights in optimized_weights.items():\n",
    "    check_zero_weights(weights, strategy)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show the stats"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the portfolio weightings for the 5 different portfolios\n",
    "def display_portfolio_weights(portfolio_name, weights, stock_names):\n",
    "    weights_df = pd.DataFrame({\n",
    "        'Stock': stock_names,\n",
    "        'Weight': weights\n",
    "    })\n",
    "    weights_df['Weight'] = weights_df['Weight'].apply(lambda x: f\"{x:.4f}\")\n",
    "    print(f\"\\nPortfolio Weights - {portfolio_name}:\\n\", weights_df)\n",
    "\n",
    "# Assuming `stocklist_mag` contains the list of stock names\n",
    "stock_names = stocklist_mag  # Replace with the correct list of stock names\n",
    "\n",
    "# Portfolio 1: Monte Carlo Simulation (Combined Weights)\n",
    "display_portfolio_weights(\"Monte Carlo Simulation (Combined)\", combined_weights, stock_names)\n",
    "\n",
    "# Portfolio 2: Combined Professional Optimizations\n",
    "display_portfolio_weights(\"Combined Professional Optimizations\", combined_optimized_weights, stock_names)\n",
    "\n",
    "# Portfolio 3: Machine Learning Optimization\n",
    "display_portfolio_weights(\"Machine Learning Optimization\", ml_optimized_weights, stock_names)\n",
    "\n",
    "# Portfolio 4: LSTM Optimization\n",
    "display_portfolio_weights(\"LSTM Optimization\", lstm_optimized_weights, stock_names)\n",
    "\n",
    "# Portfolio 5: Prophet Optimization\n",
    "display_portfolio_weights(\"Prophet Optimization\", prophet_optimized_weights, stock_names)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 8: Calculate major statistics for each portfolio and shares purchased\n",
    "\n",
    "# Function to calculate portfolio statistics\n",
    "def calculate_portfolio_stats(portfolio_values, returns, weights, stock_data, initial_investment, risk_free_rate):\n",
    "    portfolio_returns = portfolio_values.pct_change().dropna()\n",
    "\n",
    "    # Annualized Return\n",
    "    annualized_return = np.mean(portfolio_returns) * 252\n",
    "\n",
    "    # Annualized Volatility\n",
    "    annualized_volatility = np.std(portfolio_returns) * np.sqrt(252)\n",
    "\n",
    "    # Sharpe Ratio\n",
    "    sharpe_ratio = (annualized_return - risk_free_rate) / annualized_volatility\n",
    "\n",
    "    # Calculate shares purchased\n",
    "    investment_per_stock = initial_investment * weights\n",
    "    shares_purchased = investment_per_stock / stock_data.iloc[0]\n",
    "\n",
    "    return {\n",
    "        \"Annualized Return\": annualized_return,\n",
    "        \"Annualized Volatility\": annualized_volatility,\n",
    "        \"Sharpe Ratio\": sharpe_ratio,\n",
    "        \"Shares Purchased\": shares_purchased\n",
    "    }\n",
    "\n",
    "# Print statistics and shares purchased for each portfolio\n",
    "\n",
    "portfolio_stats = {}\n",
    "portfolios_to_check = {\n",
    "    \"Portfolio 1 (Monte Carlo)\": combined_portfolio_value,\n",
    "    \"Portfolio 2 (Combined Professional)\": optimized_portfolio_values['Combined Professional'],\n",
    "    \"Portfolio 3 (Machine Learning)\": optimized_portfolio_values['ML Optimized'],\n",
    "    \"Portfolio 4 (LSTM)\": optimized_portfolio_values['LSTM Optimized'],\n",
    "    \"portfolio 5 (Prophet)\": optimized_portfolio_values['Prophet Optimized']\n",
    "}\n",
    "\n",
    "for portfolio_name, portfolio_value in portfolios_to_check.items():\n",
    "    stats = calculate_portfolio_stats(portfolio_value, returns, combined_weights, backtest_data, initial_investment, daily_risk_free_rate)\n",
    "    portfolio_stats[portfolio_name] = stats\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n{portfolio_name}:\")\n",
    "    print(f\"Annualized Return: {stats['Annualized Return']:.2%}\")\n",
    "    print(f\"Annualized Volatility: {stats['Annualized Volatility']:.2%}\")\n",
    "    print(f\"Sharpe Ratio: {stats['Sharpe Ratio']:.2f}\")\n",
    "    \n",
    "    # Print shares purchased\n",
    "    print(\"Shares Purchased per Stock:\")\n",
    "    for stock, shares in zip(stocklist_mag, stats['Shares Purchased']):\n",
    "        print(f\"{stock}: {shares:.4f} shares\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_portfolio_stats(portfolio_values, returns, weights, stock_data, initial_investment, risk_free_rate, market_returns):\n",
    "    portfolio_returns = portfolio_values.pct_change().dropna()\n",
    "\n",
    "    # Annualized Return\n",
    "    annualized_return = np.mean(portfolio_returns) * 252\n",
    "\n",
    "    # Annualized Volatility\n",
    "    annualized_volatility = np.std(portfolio_returns) * np.sqrt(252)\n",
    "\n",
    "    # Sharpe Ratio\n",
    "    sharpe_ratio = (annualized_return - risk_free_rate) / annualized_volatility\n",
    "\n",
    "    # Sortino Ratio\n",
    "    downside_risk = np.std(portfolio_returns[portfolio_returns < 0]) * np.sqrt(252)\n",
    "    sortino_ratio = (annualized_return - risk_free_rate) / downside_risk if downside_risk != 0 else np.nan\n",
    "\n",
    "    # Maximum Drawdown\n",
    "    rolling_max = portfolio_values.cummax()\n",
    "    drawdown = (portfolio_values - rolling_max) / rolling_max\n",
    "    max_drawdown = drawdown.min()\n",
    "\n",
    "    # Calmar Ratio\n",
    "    calmar_ratio = annualized_return / abs(max_drawdown) if max_drawdown != 0 else np.nan\n",
    "\n",
    "    # Beta (Sensitivity to the market return)\n",
    "    covariance = np.cov(portfolio_returns, market_returns)[0, 1]\n",
    "    beta = covariance / np.var(market_returns)\n",
    "\n",
    "    # Treynor Ratio\n",
    "    treynor_ratio = (annualized_return - risk_free_rate) / beta if beta != 0 else np.nan\n",
    "\n",
    "    # Calculate shares purchased\n",
    "    investment_per_stock = initial_investment * weights\n",
    "    shares_purchased = investment_per_stock / stock_data\n",
    "\n",
    "    # Calculate the value of each stock in the portfolio (current price * shares purchased)\n",
    "    stock_values = shares_purchased * stock_data\n",
    "\n",
    "    # Calculate the percentage of the portfolio value represented by each stock\n",
    "    total_portfolio_value = stock_values.sum()\n",
    "    stock_percentages = (stock_values / total_portfolio_value) * 100\n",
    "\n",
    "    return {\n",
    "        \"Annualized Return\": annualized_return,\n",
    "        \"Annualized Volatility\": annualized_volatility,\n",
    "        \"Sharpe Ratio\": sharpe_ratio,\n",
    "        \"Sortino Ratio\": sortino_ratio,\n",
    "        \"Maximum Drawdown\": max_drawdown,\n",
    "        \"Calmar Ratio\": calmar_ratio,\n",
    "        \"Beta\": beta,\n",
    "        \"Treynor Ratio\": treynor_ratio,\n",
    "        \"Shares Purchased\": shares_purchased,\n",
    "        \"Stock Values\": stock_values,\n",
    "        \"Stock Percentages\": stock_percentages\n",
    "    }\n",
    "\n",
    "# Assuming market_return is defined (e.g., from S&P 500 or another market index)\n",
    "for portfolio_name, portfolio_value in portfolios_to_check.items():\n",
    "    stats = calculate_portfolio_stats(portfolio_value, returns, combined_weights, latest_stock_data, initial_investment, daily_risk_free_rate, market_returns)\n",
    "    portfolio_stats[portfolio_name] = stats\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"\\n{portfolio_name}:\")\n",
    "    print(f\"Annualized Return: {stats['Annualized Return']:.2%}\")\n",
    "    print(f\"Annualized Volatility: {stats['Annualized Volatility']:.2%}\")\n",
    "    print(f\"Sharpe Ratio: {stats['Sharpe Ratio']:.2f}\")\n",
    "    print(f\"Sortino Ratio: {stats['Sortino Ratio']:.2f}\")\n",
    "    print(f\"Maximum Drawdown: {stats['Maximum Drawdown']:.2%}\")\n",
    "    print(f\"Calmar Ratio: {stats['Calmar Ratio']:.2f}\")\n",
    "    print(f\"Beta: {stats['Beta']:.2f}\")\n",
    "    print(f\"Treynor Ratio: {stats['Treynor Ratio']:.2f}\")\n",
    "\n",
    "    # Print shares purchased and their value\n",
    "    print(\"Shares Purchased and Value per Stock:\")\n",
    "    for stock, shares, value, percentage in zip(stocklist_mag, stats['Shares Purchased'], stats['Stock Values'], stats['Stock Percentages']):\n",
    "        print(f\"{stock}: {shares:.4f} shares, Value: ${value:.2f}, Percentage of Portfolio: {percentage:.2f}%\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction method 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import zipfile\n",
    "import io\n",
    "import requests\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Step 1: Download the Fama-French data\n",
    "url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_CSV.zip\"\n",
    "response = requests.get(url)\n",
    "zipped_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Read the Fama-French CSV and skip the non-data rows\n",
    "with zipped_file.open('F-F_Research_Data_Factors.CSV') as ff_file:\n",
    "    fama_french_data = pd.read_csv(ff_file, skiprows=3)\n",
    "\n",
    "# Step 2: Clean the data by removing the footer and extra rows\n",
    "# Find the row with the first instance of \"Annual Factors\" (which marks the start of non-data rows)\n",
    "stop_index = fama_french_data[fama_french_data.iloc[:, 0].str.contains(\"Annual Factors\", na=False)].index[0]\n",
    "fama_french_data = fama_french_data[:stop_index]  # Only keep data rows before this index\n",
    "\n",
    "# Rename columns to appropriate labels\n",
    "fama_french_data.columns = ['Date', 'Mkt-RF', 'SMB', 'HML', 'RF']\n",
    "\n",
    "# Ensure the Date column is correctly formatted (skip rows with any non-date value)\n",
    "fama_french_data['Date'] = pd.to_datetime(fama_french_data['Date'], format='%Y%m', errors='coerce').dropna()\n",
    "\n",
    "# Set the date as index\n",
    "fama_french_data = fama_french_data.set_index('Date')\n",
    "\n",
    "# Convert percentage values to decimals for easier calculation\n",
    "fama_french_data = fama_french_data.astype(float) / 100  # Convert percentages to decimals\n",
    "\n",
    "# Display the first few rows to ensure the cleaning is correct\n",
    "print(fama_french_data.head())\n",
    "\n",
    "\n",
    "\n",
    "end_date = datetime.today()  # Keep this as a datetime object\n",
    "one_year_ago = end_date - timedelta(days=365)\n",
    "# Filter the data to match our stock period\n",
    "start_date_str = one_year_ago.strftime('%Y-%m')  # Convert the datetime object to string format\n",
    "end_date_str = end_date.strftime('%Y-%m')  # Convert the datetime object to string format\n",
    "fama_french_filtered = fama_french_data.loc[start_date_str:end_date_str]\n",
    "\n",
    "# Prepare the market returns (Market - RF) and risk-free rate\n",
    "market_excess_return = fama_french_filtered['Mkt-RF']\n",
    "risk_free_rate_fama = fama_french_filtered['RF']\n",
    "SMB = fama_french_filtered['SMB']\n",
    "HML = fama_french_filtered['HML']\n",
    "\n",
    "risk_free_rate_fama = risk_free_rate_fama.tz_localize(None)\n",
    "\n",
    "\n",
    "# Step 2: Calculate the excess returns of the portfolios\n",
    "portfolios_excess_returns = {}\n",
    "for portfolio_name, portfolio_value in portfolios_to_check.items():\n",
    "    # Calculate excess returns (portfolio returns - risk-free rate)\n",
    "    portfolio_returns = portfolio_value.pct_change().dropna()\n",
    "    # Remove timezone information from the index\n",
    "    portfolio_returns.index = portfolio_returns.index.tz_localize(None)\n",
    "\n",
    "    portfolio_excess_return = portfolio_returns - risk_free_rate_fama.reindex(portfolio_returns.index, method='ffill')\n",
    "    portfolios_excess_returns[portfolio_name] = portfolio_excess_return\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Step 3: CAPM Regression (for each portfolio)\n",
    "def capm_prediction(excess_returns, market_excess_return, risk_free_rate_fama):\n",
    "    # Align indices and remove NaNs\n",
    "    excess_returns, market_excess_return = excess_returns.align(market_excess_return, join='inner')\n",
    "    \n",
    "    # Reshape data for regression\n",
    "    X = market_excess_return.values.reshape(-1, 1)\n",
    "    y = excess_returns.values\n",
    "    \n",
    "    # Remove NaNs (if any) in case of data misalignment\n",
    "    valid_mask = ~np.isnan(X).flatten() & ~np.isnan(y)\n",
    "    X = X[valid_mask]\n",
    "    y = y[valid_mask]\n",
    "    \n",
    "    # Perform linear regression\n",
    "    capm_model = LinearRegression()\n",
    "    capm_model.fit(X, y)\n",
    "    \n",
    "    # Beta coefficient\n",
    "    beta = capm_model.coef_[0]\n",
    "    \n",
    "    # Predict the next 3 months using the average market return\n",
    "    future_market_return = market_excess_return.mean()  # Using historical average\n",
    "    expected_return = risk_free_rate_fama.mean() + beta * future_market_return\n",
    "    return expected_return, beta\n",
    "\n",
    "capm_predictions = {}\n",
    "for portfolio_name, excess_returns in portfolios_excess_returns.items():\n",
    "    expected_return, beta = capm_prediction(excess_returns, market_excess_return, risk_free_rate_fama)\n",
    "    capm_predictions[portfolio_name] = (expected_return, beta)\n",
    "\n",
    "# Step 4: Fama-French Regression (for each portfolio)\n",
    "def fama_french_prediction(excess_returns, market_excess_return, SMB, HML, risk_free_rate_fama):\n",
    "    # Align indices and remove NaNs\n",
    "    excess_returns, market_excess_return = excess_returns.align(market_excess_return, join='inner')\n",
    "    excess_returns, SMB = excess_returns.align(SMB, join='inner')\n",
    "    excess_returns, HML = excess_returns.align(HML, join='inner')\n",
    "    \n",
    "    # Combine the Fama-French factors into a single DataFrame and drop NaNs\n",
    "    X = pd.DataFrame({\n",
    "        'Mkt-RF': market_excess_return,\n",
    "        'SMB': SMB,\n",
    "        'HML': HML\n",
    "    }).dropna()\n",
    "    y = excess_returns.loc[X.index].values\n",
    "    \n",
    "    # Perform multivariate linear regression\n",
    "    fama_french_model = LinearRegression()\n",
    "    fama_french_model.fit(X, y)\n",
    "    \n",
    "    # Coefficients (betas)\n",
    "    beta_market = fama_french_model.coef_[0]\n",
    "    beta_smb = fama_french_model.coef_[1]\n",
    "    beta_hml = fama_french_model.coef_[2]\n",
    "    \n",
    "    # Predict the next 3 months using the average factors\n",
    "    future_market_return = market_excess_return.mean()  # Using historical average\n",
    "    future_SMB = SMB.mean()\n",
    "    future_HML = HML.mean()\n",
    "    \n",
    "    expected_return = risk_free_rate_fama.mean() + beta_market * future_market_return + beta_smb * future_SMB + beta_hml * future_HML\n",
    "    return expected_return, (beta_market, beta_smb, beta_hml)\n",
    "\n",
    "fama_french_predictions = {}\n",
    "for portfolio_name, excess_returns in portfolios_excess_returns.items():\n",
    "    expected_return, betas = fama_french_prediction(excess_returns, market_excess_return, SMB, HML, risk_free_rate_fama)\n",
    "    fama_french_predictions[portfolio_name] = (expected_return, betas)\n",
    "\n",
    "# Step 5: Print the predictions for the next 3 months\n",
    "print(\"CAPM Predictions (Next 3 months):\")\n",
    "for portfolio_name, (expected_return, beta) in capm_predictions.items():\n",
    "    print(f\"{portfolio_name}: Expected Return: {expected_return:.4%}, Beta: {beta:.4f}\")\n",
    "\n",
    "print(\"\\nFama-French Predictions (Next 3 months):\")\n",
    "for portfolio_name, (expected_return, betas) in fama_french_predictions.items():\n",
    "    beta_market, beta_smb, beta_hml = betas\n",
    "    print(f\"{portfolio_name}: Expected Return: {expected_return:.4%}, Beta_Market: {beta_market:.4f}, Beta_SMB: {beta_smb:.4f}, Beta_HML: {beta_hml:.4f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
